<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>til</title>
<link>https://lawwu.github.io/til/</link>
<atom:link href="https://lawwu.github.io/til/index.xml" rel="self" type="application/rss+xml"/>
<description>Lawrence Wu&#39;s Today I Learned</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Wed, 02 Oct 2024 00:00:00 GMT</lastBuildDate>
<item>
  <title>Whisper large-v3-turbo</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-10-02-whisper-v3-turbo/</link>
  <description><![CDATA[ 





<p>OpenAI released a new version of their Whisper model caled <code>large-v3-turbo</code>. There is already <code>ggml</code> versions available <a href="https://huggingface.co/ggerganov/whisper.cpp/tree/main">here</a>.</p>
<ul>
<li><code>ggml-large-v3-turbo.bin</code></li>
<li><code>ggml-large-v3-turbo-q5_0.bin</code></li>
</ul>
<p>HuggingFace has a model card <a href="https://huggingface.co/openai/whisper-large-v3-turbo">here</a>. There’s a Github discussion <a href="https://github.com/openai/whisper/discussions/2363">here</a> talking about the model. It’s basically a distilled version of <code>large-v3</code>:</p>
<pre><code>We’re releasing a new Whisper model named large-v3-turbo, or turbo for short. It is an optimized version of Whisper large-v3 and has only 4 decoder layers—just like the tiny model—down from the 32 in the large series.

This work is inspired by Distil-Whisper, where the authors observed that using a smaller decoder can greatly improve transcription speed while causing minimal degradation in accuracy. Unlike Distil-Whisper, which used distillation to train a smaller model, Whisper turbo was fine-tuned for two more epochs over the same amount of multilingual transcription data used for training large-v3, i.e. excluding translation data, on which we don’t expect turbo to perform well.

Across languages, the turbo model performs similarly to large-v2, though it shows larger degradation on some languages like Thai and Cantonese. Whisper turbo performs better on FLEURS, which consists of cleaner recordings than Common Voice. The figure below shows the turbo model’s performance on the subset of languages in the Common Voice 15 and FLEURS datasets where large-v3 scored a 20% error rate or lower.</code></pre>
<p>I started using this model in my <a href="https://github.com/lawwu/transcripts/commit/4385305b1bf928e46d9f323cd1d4295dd231add8">transcripts repo</a>.</p>



 ]]></description>
  <category>whisper</category>
  <category>llm</category>
  <category>asr</category>
  <guid>https://lawwu.github.io/til/posts/2024-10-02-whisper-v3-turbo/</guid>
  <pubDate>Wed, 02 Oct 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Juan Carlos Izpisua Belmonte &amp; Yamanaka Factors</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-10-03-yamanaka-factors/</link>
  <description><![CDATA[ 





<p>I was watching one of the videos from the All-In Summit 2024:</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/uUjF3RG28SQ" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<p>Sometimes YouTube videos are too long so I will grab the <a href="https://lawwu.github.io/transcripts/transcript_uUjF3RG28SQ.html">transcript</a>, also possible to do <a href="https://github.com/jdepoix/youtube-transcript-api">programmatically</a> or via web tools like this <a href="https://youtubetranscript.com/">one</a>. And then I’ll drop it into ChatGPT with a prompt: “What are the main points and takeaways from this talk?”. Example <a href="https://chatgpt.com/share/66fda61a-5d8c-8011-a679-eebeb0c51d35">here</a>.</p>
<p>Aging and Disease Connection: The speaker, Juan Carlos Izpisua Belmonte, explains how age is the biggest risk factor for many diseases. As we age, cells lose their resilience or “buffer capacity,” making them more vulnerable to disease.</p>
<ul>
<li>Yamanaka Factors and Cell Reprogramming: Yamanaka’s discovery of four proteins (Yamanaka factors) that can reprogram adult cells into embryonic-like stem cells plays a crucial role. The focus now is on “partial reprogramming,” where a short pulse of these factors can rejuvenate cells without causing them to lose their identity (which could otherwise lead to cancer or malfunction).</li>
<li>Chromatin and the Epigenome: Belmonte explains how chromatin (DNA packaging) can either be open (leading to aging and disease) or closed (correlating with health and youth). Altering the chromatin to a more closed state can rejuvenate cells and potentially reverse disease.</li>
<li>Application Beyond Mice: This rejuvenation technology has been successfully tested in mice, including fixing genetic mutations and extending the lifespan of diseased mice. The next step is to apply these findings to human cells and organs, focusing on improving their resilience to disease without altering their genome.</li>
<li>Potential for Human Applications: Altos Labs aims to apply these techniques in humans, starting with ex vivo organ rejuvenation (e.g., preparing discarded organs for transplantation). The ultimate goal is to develop methods that can increase human cells’ buffer capacity, thereby extending health span and delaying or reversing aging-related diseases.</li>
<li>Future of Rejuvenation Therapies: The talk emphasizes the long-term potential of partial cell reprogramming to revolutionize human health by combating diseases linked to aging. The approach is described as a combination of cutting-edge science and industry, with billions of dollars invested in fundamental research and safe applications for human health.</li>
<li>Chamath asks whether lifestyle choices like exercise can slow the aging process or improve cellular buffer capacity. Belmonte reveals that the gene expression changes induced by exercise resemble those triggered by the short pulses of Yamanaka factors, showing that lifestyle interventions can have a significant impact on cellular health and longevity.</li>
</ul>
<p>I found it fascinating that cellular aging could be reversed. The easiest way we have access to that is through simple exercise which triggers gene expression changes induced by short pulses of Yamanka factors!</p>



 ]]></description>
  <category>aging</category>
  <category>exercise</category>
  <guid>https://lawwu.github.io/til/posts/2024-10-03-yamanaka-factors/</guid>
  <pubDate>Wed, 02 Oct 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Github Copilot Workspace</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-27-github-copilot-workspace/</link>
  <description><![CDATA[ 





<p>I was granted access to Github Copilot Workspace. It’s Github’s AI Coding Assistant built into Github.com where you can go from Github Issue to a Pull Request. I tried it, it’s pretty cool. The PR the service generated in my <a href="https://github.com/lawwu/transcripts">transcripts</a> repo is <a href="https://github.com/lawwu/transcripts/pull/3">here</a>. The workspace is viewable <a href="https://copilot-workspace.githubnext.com/lawwu/transcripts/issues/2?shareId=685875ce-8d8a-4c54-9376-f58bd34f89fe">here</a>.</p>
<p>The workflow is:</p>
<section id="create-an-issue" class="level2">
<h2 class="anchored" data-anchor-id="create-an-issue">1. Create an Issue</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-27-github-copilot-workspace/0_issue.png" class="img-fluid figure-img"></p>
<figcaption>Create an Issue</figcaption>
</figure>
</div>
</section>
<section id="agent-will-create-a-spec" class="level2">
<h2 class="anchored" data-anchor-id="agent-will-create-a-spec">2. Agent will create a spec</h2>
<p>That you can edit</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-27-github-copilot-workspace/1_spec.png" class="img-fluid figure-img"></p>
<figcaption>Agent will create a specification</figcaption>
</figure>
</div>
</section>
<section id="agent-will-create-a-plan" class="level2">
<h2 class="anchored" data-anchor-id="agent-will-create-a-plan">3. Agent will create a plan</h2>
<p>The agent creates a plans of what files will be edited according to your spec.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-27-github-copilot-workspace/2_plan.png" class="img-fluid figure-img"></p>
<figcaption>Agent will create a plan</figcaption>
</figure>
</div>
</section>
<section id="agent-will-create-commits" class="level2">
<h2 class="anchored" data-anchor-id="agent-will-create-commits">4. Agent will create commits</h2>
<p>The agent then creates commits and a pull request! There’s also a chat interface to make revisions which is cool.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-27-github-copilot-workspace/3_commits_with_revise.png" class="img-fluid figure-img"></p>
<figcaption>Agent will create a PR</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <category>github</category>
  <category>agent</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-27-github-copilot-workspace/</guid>
  <pubDate>Fri, 27 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Turn on Rich Jupyter diff’s in Github</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-26-github-jupyter-diffs/</link>
  <description><![CDATA[ 





<p>Github.com has different features hidden behind a “Feature Preview” menu. You can get there by clicking your profile and go to “Feature Preview.”</p>
<p>One of the options is to turn on Rich Jupyter Diff’s in Github:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-26-github-jupyter-diffs/jupyter_diffs.png" class="img-fluid figure-img"></p>
<figcaption>Rich Jupyter Diffs in Github, nice</figcaption>
</figure>
</div>
<p>This follows a trend of large companies (Microsoft who owns Githbub) using open source tooling (the diff software likely is just using <a href="https://github.com/jupyter/nbdime">nbdime</a>), making it free and making paid services like <a href="https://www.reviewnb.com/#pricing">ReviewNB</a> obsolete.</p>



 ]]></description>
  <category>github</category>
  <category>jupyter</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-26-github-jupyter-diffs/</guid>
  <pubDate>Thu, 26 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Moshi - Real-Time Voice Chatbot</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-20-moshi-real-time-chatbot/</link>
  <description><![CDATA[ 





<p>To install:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># on a M series Mac</span></span>
<span id="cb1-2"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># brew install rust</span></span>
<span id="cb1-3"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">python3</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> venv venv</span>
<span id="cb1-4"><span class="bu" style="color: null;
background-color: null;
font-style: inherit;">source</span> venv/bin/activate</span>
<span id="cb1-5"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">pip</span> install moshi_mlx</span>
<span id="cb1-6"><span class="co" style="color: #5E5E5E;
background-color: null;
font-style: inherit;"># run moshi (will download the quantized model) and you can chat with it</span></span>
<span id="cb1-7"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">python</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-m</span> moshi_mlx.local <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-q</span> 4</span></code></pre></div>
<p>I setup this up on two laptops and recorded Moshi talking to Moshi (after seeding it with a topic on basketball):</p>
<div class="quarto-video ratio ratio-16x9"><iframe data-external="1" src="https://www.youtube.com/embed/bL-hYq2F7ow" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>



 ]]></description>
  <category>llm</category>
  <category>voice</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-20-moshi-real-time-chatbot/</guid>
  <pubDate>Fri, 20 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>How to configure Apple Focus to help minimize distractions</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-19-apple-focus/</link>
  <description><![CDATA[ 





<p>Focus modes on Apple devices (iOS and MacOS) are useful. You can configure these to allow notifications from certain people and/or apps when the focus mode is on (e.g.&nbsp;allow notifications from a spouse and/or calls from your Favorites). You can also schedule these modes for example to turn on during working hours.</p>
<p>See these instructions for iOS (but will be available anywhere you are signed into your Apple account): <a href="https://support.apple.com/guide/iphone/set-up-a-focus-iphd6288a67f/ios" class="uri">https://support.apple.com/guide/iphone/set-up-a-focus-iphd6288a67f/ios</a></p>



 ]]></description>
  <category>apple</category>
  <category>tool</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-19-apple-focus/</guid>
  <pubDate>Thu, 19 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>History of Python HTTP Clients</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-17-python-http-clients/</link>
  <description><![CDATA[ 





<p>Learned about the history of Python HTTP clients: <code>urllib</code> to <code>Requests</code> to now <code>httpx</code> and <code>aiohttp</code> from this article by <a href="https://www.speakeasy.com/post/python-http-clients-requests-vs-httpx-vs-aiohttp">SpeakEasy</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-17-python-http-clients/http_clients.png" class="img-fluid figure-img"></p>
<figcaption>Python HTTP Clients</figcaption>
</figure>
</div>
<p>The above company, SpeakEasy generates SDKs from an OpenAPI spec (looks Mistral uses them). I think OpenAI uses a competitor called [Stainless](https://www.stainlessapi.com.</p>



 ]]></description>
  <category>python</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-17-python-http-clients/</guid>
  <pubDate>Tue, 17 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>alphaXiv</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-16-alphaxiv/</link>
  <description><![CDATA[ 





<p>This is a neat project: <a href="https://www.alphaxiv.org/" class="uri">https://www.alphaxiv.org/</a></p>
<p>This project from students at Stanford creates an open discussion forum for arXiv papers. You can post questions and comments directly on top of any arXiv paper by changing arXiv to alphaXiv in any URL.</p>
<p>There’s also a trending papers view: <a href="https://www.alphaxiv.org/explore" class="uri">https://www.alphaxiv.org/explore</a></p>



 ]]></description>
  <category>tool</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-16-alphaxiv/</guid>
  <pubDate>Mon, 16 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>OpenAI Releases o1-preview</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-12-o1-preview/</link>
  <description><![CDATA[ 





<ul>
<li>Model Card: <a href="https://cdn.openai.com/o1-system-card.pdf" class="uri">https://cdn.openai.com/o1-system-card.pdf</a></li>
<li>Simon Willison’s notes: <a href="https://simonwillison.net/2024/Sep/12/openai-o1/" class="uri">https://simonwillison.net/2024/Sep/12/openai-o1/</a></li>
</ul>
<section id="summary" class="level1">
<h1>Summary</h1>
<p>This model was trained on chain-of-thought chains. These were not human annotations but were LLM-generated. The CoT chains ultimately used for training were the ones which led to correct answers in questions. New ideas being coined: reasoning tokens, and inference or “test-time compute”.</p>
</section>
<section id="model-description" class="level1">
<h1>Model Description</h1>
<p>The o1 large language model family is trained with reinforcement learning to perform complex reasoning. o1 thinks before it answers—it can produce a long chain of thought before responding to the user. OpenAI o1-preview is the early version of this model, while OpenAI o1-mini is a faster version of this model that is particularly effective at coding. Through training, the models learn to refine their thinking process, try different strategies, and recognize their mistakes. Reasoning allows o1 models to follow specific guidelines and model policies we’ve set, ensuring they act in line with our safety expectations. This means they are better at providing helpful answers and resisting attempts to bypass safety rules, to avoid producing unsafe or inappropriate content. o1-preview is state-of-the-art (SOTA) on various evaluations spanning coding, math, and known jailbreaks benchmarks [1, 2, 3, 4].</p>
</section>
<section id="my-usage" class="level1">
<h1>My Usage</h1>
<p>A new output is “The model thinks for N seconds”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-12-o1-preview/o1_preview.png" class="img-fluid figure-img"></p>
<figcaption>o1-preview thinking</figcaption>
</figure>
</div>
<p>If you un-hide the menu, you can see the steps it went through. Many times the model is checking if the output meets OpenAI’s content policies. It feels like I am interacting with an agent now where the agent is:</p>
<ul>
<li>Evaluating how difficult the problem is</li>
<li>Planning out the steps - this part is really amazing and the real new aspect of these models</li>
<li>Providing the answer</li>
<li>Checking the answer</li>
</ul>
<pre><code>CHUNK:
Crafting an interactive model
I’m crafting an HTML and JavaScript model to visualize self-attention in a Transformer model, focusing on hovering and clicking functionalities, token
alignment, and LaTeX rendering for attention scores.

Crafting the code
I’m figuring out how to write HTML and JavaScript code to create an interactive visualization of Transformer self-attention, displaying tokens and
connecting them with edges based on attention scores. No external libraries allowed.

Mapping out tokenization
Mapping out the process involves outlining tokens, drawing curved, non-overlapping edges proportional to attention scores, and visualizing these scores
with CSS instead of MathJax.

Structuring HTML with attention scores
I'm thinking about how to display tokens in HTML using spans and defining a 4x4 attention score matrix. Each token's focus on others should sum to 1,
ensuring clarity and precision.

Mapping attention
I'm working through visualizing how attention flows from "The" to other tokens in a sentence. The edges, curved for clarity, will show proportional thickness
and start/end at token centers, avoiding overlap.

Setting up the stage
I’m working through creating an HTML structure with 50px spacing at the top, presenting tokens, indicating edges on hover, and showing attention scores
on click, all while ensuring clarity and alignment.

Setting up the layout
I’m beginning with a minimal HTML structure, adding CSS for styling (like margin, padding, font size) and positioning. This foundation sets up token and
edge elements for attention scores.

Setting up event handling
I’m defining attention scores and referencing SVG elements. Tokens get mouseover, mouseout, and click event listeners, with functions to show/hide
attention scores and draw/clear SVG edges.

Tracing connections
I’m detailing how to visualize and manage connections among tokens, their attention scores, and paths in SVG format. This involves removing old edges,
adding new paths, and updating attention score displays.

Enhancing table layout
I’m adding CSS to style the table, centering it and adjusting padding and alignment. Next, I’ll tackle overlapping paths and curved edges for clarity.
Tweaking the plot
OK, let me see. I’m enhancing the drawEdges function to incorporate quadratic Bezier curves, aiming to boost graph visualization.

Enhancing edges
I’m thinking through the drawEdges function to add quadratic Bezier curves. The function dynamically adjusts stroke widths based on attention scores.

Refining curvature adjustment
I'm digging into the idea of tweaking curve profiles by matching curvature changes with index differences to avoid overlaps. Considering control point shifts
or even slight overlaps might be practical due to time limits.

Improving map functionality
I'm working on the function, enhancing control points to prevent paths from overlapping. By clearing existing edges and mapping tokens to drawEdges
their respective SVG elements, I’m reducing intersections and breaking up any overlap.

Presenting attention scores
I'm considering how to visually represent attention scores, aiming for neutrality and alignment. Utilizing a table with two columns for tokens and scores
seems like the best approach.

Ensuring table clarity
I'm thinking through the use of a table for neatness, focusing on CSS for token and score alignment. Hover effects and consistent vertical spacing are
considered.

Showcasing the code
I’m starting with a clear HTML structure, setting up styles and an SVG for edges. The script shows attention scores on token interaction, utilizing mouse
events.</code></pre>
<section id="reasoning-tokens" class="level2">
<h2 class="anchored" data-anchor-id="reasoning-tokens">Reasoning Tokens</h2>
<p>Questions on reasoning tokens - Are these using a different vocabulary from the wider model? - This does make OpenAI reasoning based models more black boxy.</p>
<p><a href="https://platform.openai.com/docs/guides/reasoning/how-reasoning-works" class="uri">https://platform.openai.com/docs/guides/reasoning/how-reasoning-works</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-12-o1-preview/o1_reasoning_tokens.png" class="img-fluid figure-img"></p>
<figcaption>o1-preview reasoning tokens</figcaption>
</figure>
</div>
</section>
<section id="ai-explained-review" class="level2">
<h2 class="anchored" data-anchor-id="ai-explained-review">AI Explained Review</h2>
<p>On AI Explained’s simple-bench benchmark, o1-preview does seem like a step change from the previous generation of models.</p>
<p>o1-preview is getting ~50% compared to other models 27% or less (though temperature was 1.0 when tested which is higher than other models)</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-12-o1-preview/simple_bench_o1.png" class="img-fluid figure-img"></p>
<figcaption>o1-preview on simple bench</figcaption>
</figure>
</div>
<p>He think it is a new paradigm based on the step change improvement across a wide variety of domains and his benchmark Simple Bench.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-12-o1-preview/o1_benchmarks.png" class="img-fluid figure-img"></p>
<figcaption>o1-preview benchmarks</figcaption>
</figure>
</div>
<p>o1 outperforms expert humans in GPQA Diamond (PhD-Level Science questions)!</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-09-12-o1-preview/o1_reasoning.png" class="img-fluid figure-img"></p>
<figcaption>o1-preview beats expert humans</figcaption>
</figure>
</div>


</section>
</section>

 ]]></description>
  <category>llm</category>
  <category>reasoning</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-12-o1-preview/</guid>
  <pubDate>Thu, 12 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Learn-to-Rank and Learning with ChatGPT</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-09-11-learn-to-rank/</link>
  <description><![CDATA[ 





<p>I recently learned about algorithms called Learn-to-Rank (LTR) from a talk that Doug Turnbell gave (he works at Reddit). I actually didn’t know they existed. “LTR models are specifically designed for ranking tasks, optimizing metrics like NDCG or MAP, which are more aligned with ranking quality.” Also learned that xgboost has an implementation of LTR here: <a href="https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html" class="uri">https://xgboost.readthedocs.io/en/stable/tutorials/learning_to_rank.html</a>.</p>
<p>I then had a <a href="https://chatgpt.com/share/c973b294-64a9-42d3-892e-3bced47891c6">conversation with ChatGPT</a> on how this could be applied to Candidate Match. (Side note: I find LLMs to be fantastic for this type of thinking/research, I copy pasted the xgboost LTR documentation into the prompt for context).</p>



 ]]></description>
  <category>learning</category>
  <guid>https://lawwu.github.io/til/posts/2024-09-11-learn-to-rank/</guid>
  <pubDate>Wed, 11 Sep 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Amphetamine/Caffeine Replacement</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-30-caffiene-replacement/</link>
  <description><![CDATA[ 





<p>Mac OSX Applications like Caffeine and now Amphetamine are simple apps that cause them not to go to sleep.</p>
<p>You can also use a simple terminal command:</p>
<p>Open a terminal and type:</p>
<div class="sourceCode" id="cb1" style="background: #f1f3f5;"><pre class="sourceCode bash code-with-copy"><code class="sourceCode bash"><span id="cb1-1"><span class="ex" style="color: null;
background-color: null;
font-style: inherit;">caffeinate</span> <span class="at" style="color: #657422;
background-color: null;
font-style: inherit;">-d</span></span></code></pre></div>
<p>h/t <a href="https://www.reddit.com/r/macapps/comments/tgxtnu/is_amphetamine_still_the_best_app_to_prevent_sleep/">Reddit</a></p>



 ]]></description>
  <category>utility</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-30-caffiene-replacement/</guid>
  <pubDate>Fri, 30 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>great-tables Python package</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-29-greattables/</link>
  <description><![CDATA[ 





<p>Repo from posit to create beautiful tables in Python: <a href="https://github.com/posit-dev/great-tables" class="uri">https://github.com/posit-dev/great-tables</a>. Basically the Python version of R’s <a href="https://github.com/rstudio/gt" class="uri">https://github.com/rstudio/gt</a>. I love how posit is now contributing to the Python open source ecosystem.</p>



 ]]></description>
  <category>python</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-29-greattables/</guid>
  <pubDate>Thu, 29 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>CogVideoX-5B aka Open source Sora</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-28-opensource-sora/</link>
  <description><![CDATA[ 





<p>It’s neat how when OpenAI or another model provider creates a new model (e.g.&nbsp;text-to-video like Sora), the open source community gets to work and produces an open source version. This happened with Sora (although the quality is not quite there yet):</p>
<ul>
<li>Space: <a href="https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space" class="uri">https://huggingface.co/spaces/THUDM/CogVideoX-5B-Space</a></li>
<li>Model Card: <a href="https://huggingface.co/THUDM/CogVideoX-5b" class="uri">https://huggingface.co/THUDM/CogVideoX-5b</a></li>
</ul>



 ]]></description>
  <category>llm</category>
  <category>agent</category>
  <category>video</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-28-opensource-sora/</guid>
  <pubDate>Wed, 28 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>RunnablePassthrough.assign</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-27-runnablepassthrough/</link>
  <description><![CDATA[ 





<p>I’m learning more about LangGraph and I didn’t know what <code>RunnablePassthrough.assign</code> was doing (I’ve seen this in some of the LangGraph tutorials). This page explains it really well: <a href="https://python.langchain.com/v0.1/docs/expression_language/primitives/assign/" class="uri">https://python.langchain.com/v0.1/docs/expression_language/primitives/assign/</a></p>



 ]]></description>
  <category>llm</category>
  <category>agent</category>
  <category>langgraph</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-27-runnablepassthrough/</guid>
  <pubDate>Tue, 27 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>WebVoyager implemented in LangGraph</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-26-webvoyager/</link>
  <description><![CDATA[ 





<p>Really cool agent that can navigate the web. It uses Playwright to take screenshots, annotates those screenshot images with bounding boxes of parts of the webpage that are interactable and then generates the next best action (click, scroll, wait, type). Implemented in LangGraph. I learned about <code>ImagePromptTemplate</code> from this tutorial. I originally thought langchain didn’t handle images well.</p>
<p><a href="https://langchain-ai.github.io/langgraph/tutorials/web-navigation/web_voyager/" class="uri">https://langchain-ai.github.io/langgraph/tutorials/web-navigation/web_voyager/</a></p>
<p>The prompt is available in the LangChain Hub <a href="https://smith.langchain.com/hub/wfh/web-voyager">here</a>:</p>
<pre><code>from typing import List, Union, Dict
from langchain_core.messages import AIMessage, HumanMessage, ChatMessage, SystemMessage, FunctionMessage,
ToolMessage
from langchain_core.prompts import SystemMessagePromptTemplate, MessagesPlaceholder,
HumanMessagePromptTemplate, PromptTemplate
from langchain_core.prompts.image import ImagePromptTemplate
from langchain_core.prompts.chat import ChatPromptTemplate
def create_chat_prompt_template() -&gt; ChatPromptTemplate:
input_variables = ['bbox_descriptions', 'img', 'input']
optional_variables = ['scratchpad']
input_types = {
'scratchpad': List[Union[
AIMessage,
HumanMessage,
ChatMessage,
SystemMessage,
FunctionMessage,
ToolMessage
]]
}
partial_variables = {'scratchpad': []}
# metadata = {
# 'lc_hub_owner': 'wfh',
# 'lc_hub_repo': 'web-voyager',
# 'lc_hub_commit_hash': '8b9276048be8aec78203e8c45c9e15bc3a46e4b3275b05ef727563a2887ebaab'
# }
system_prompt = SystemMessagePromptTemplate(
prompt=[
PromptTemplate(
input_variables=[],
template="""
Imagine you are a robot browsing the web, just like humans. Now you need to complete a task. In each iteration, you will receive an Observation that includes a screenshot of a webpage and some texts. This screenshot will feature Numerical Labels placed in the TOP LEFT corner of each Web Element. Carefully analyze the visual
information to identify the Numerical Label corresponding to the Web Element that requires interaction, then follow
the guidelines and choose one of the following actions:
1. Click a Web Element.
2. Delete existing content in a textbox and then type content.
3. Scroll up or down.
4. Wait 
5. Go back
7. Return to google to start over.
8. Respond with the final answer

Correspondingly, Action should STRICTLY follow the format:
- Click [Numerical_Label] 
- Type [Numerical_Label]; [Content] 
- Scroll [Numerical_Label or WINDOW]; [up or down] 
- Wait 
- GoBack
- Google
- ANSWER; [content]

Key Guidelines You MUST follow:
* Action guidelines *
1) Execute only one action per iteration.
2) When clicking or typing, ensure to select the correct bounding box.
3) Numeric labels lie in the top-left corner of their corresponding bounding boxes and are colored the same.

* Web Browsing Guidelines *
1) Don't interact with useless web elements like Login, Sign-in, donation that appear in Webpages
2) Select strategically to minimize time wasted.

Your reply should strictly follow the format:
Thought: {{Your brief thoughts (briefly summarize the info that will help ANSWER)}}
Action: {{One Action format you choose}}
Then the User will provide:
Observation: {{A labeled screenshot Given by User}}
"""
)
]
)
human_prompt = HumanMessagePromptTemplate(
prompt=[
ImagePromptTemplate(
input_variables=['img'],
template={'url': 'data:image/png;base64,{img}'}
),
PromptTemplate(
input_variables=['bbox_descriptions'],
template='{bbox_descriptions}'
),
PromptTemplate(
input_variables=['input'],
template='{input}'
)
]
)
messages = [
system_prompt,
MessagesPlaceholder(variable_name='scratchpad', optional=True),
human_prompt
]
return ChatPromptTemplate(
input_variables=input_variables,
optional_variables=optional_variables,
input_types=input_types,
partial_variables=partial_variables,
# metadata=metadata,
messages=messages
)
prompt = create_chat_prompt_template()</code></pre>



 ]]></description>
  <category>llm</category>
  <category>agent</category>
  <category>langgraph</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-26-webvoyager/</guid>
  <pubDate>Mon, 26 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>ColiPali: Efficient Document Retrieval with Vision Language Models</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-16-colipali/</link>
  <description><![CDATA[ 





<p>Learned about ColiPali via <a href="https://x.com/tonywu_71/status/1824194413469503764?s=12">Twitter/X</a>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-08-16-colipali/colipali_tweet.png" class="img-fluid figure-img"></p>
<figcaption>Tony Wu on ColiPali</figcaption>
</figure>
</div>
<ul>
<li>Blog Post: <a href="https://huggingface.co/blog/manu/colpali" class="uri">https://huggingface.co/blog/manu/colpali</a></li>
<li>Demo: <a href="https://huggingface.co/spaces/manu/ColPali-demo" class="uri">https://huggingface.co/spaces/manu/ColPali-demo</a></li>
<li>Model: <a href="https://huggingface.co/vidore/colpali" class="uri">https://huggingface.co/vidore/colpali</a></li>
<li>Then in September 2024, Ben Clavie released the <a href="https://github.com/AnswerDotAI/byaldi"><code>byaldi</code></a> to make it easier to use ColiPali models. Haven’t used it yet but it sounds like the equivalent of what <a href="https://github.com/AnswerDotAI/RAGatouille">RAGatouille</a> did for <a href="https://github.com/stanford-futuredata/ColBERT">ColBERT</a>.</li>
</ul>



 ]]></description>
  <category>llm</category>
  <category>vision</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-16-colipali/</guid>
  <pubDate>Fri, 16 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>LlamaCoder</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-15-llamacoder/</link>
  <description><![CDATA[ 





<p><a href="https://llamacoder.together.ai/">LlamaCoder</a> is an AI agent that takes natural language and generates a React app. It’s billed as an “open source version of Claude Artifacts.” The Github repo is <a href="https://github.com/Nutlope/llamacoder">here</a>.</p>
<p>It uses llama 3.1 405B. I built a snake game that actually works… incredible.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-08-15-llamacoder/snake.png" class="img-fluid figure-img"></p>
<figcaption>Snake Game</figcaption>
</figure>
</div>
<p>The <a href="https://github.com/Nutlope/llamacoder/blob/bdf2a059ab30b620ec47855ddc6eac985d490fea/app/api/generateCode/route.ts#L86">system prompt</a> is interesting. “I will tip you $1 million if you do a good job” 😂</p>
<pre><code>You are an expert frontend React engineer who is also a great UI/UX designer. Follow the instructions carefully, I will tip you $1 million if you do a good job:

- Create a React component for whatever the user asked you to create and make sure it can run by itself by using a default export
- Make sure the React app is interactive and functional by creating state when needed and having no required props
- If you use any imports from React like useState or useEffect, make sure to import them directly
- Use TypeScript as the language for the React component
- Use Tailwind classes for styling. DO NOT USE ARBITRARY VALUES (e.g. \`h-[600px]\`). Make sure to use a consistent color palette.
- Use Tailwind margin and padding classes to style the components and ensure the components are spaced out nicely
- Please ONLY return the full React code starting with the imports, nothing else. It's very important for my job that you only return the React code with imports. DO NOT START WITH \`\`\`typescript or \`\`\`javascript or \`\`\`tsx or \`\`\`.
- ONLY IF the user asks for a dashboard, graph or chart, the recharts library is available to be imported, e.g. \`import { LineChart, XAxis, ... } from "recharts"\` &amp; \`&lt;LineChart ...&gt;&lt;XAxis dataKey="name"&gt; ...\`. Please only use this when needed.
`;

// Removed because it causes too many errors
// - The lucide-react@0.263.1 library is also available to be imported. If you need an icon, use one from lucide-react. Here's an example of importing and using one: import { Camera } from "lucide-react"\` &amp; \`&lt;Camera color="red" size={48} /&gt;\`
</code></pre>



 ]]></description>
  <category>llm</category>
  <category>agent</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-15-llamacoder/</guid>
  <pubDate>Thu, 15 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Excalidraw</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-08-13-excalidraw/</link>
  <description><![CDATA[ 





<p>I didn’t realize <a href="https://excalidraw.com/">Excalidraw</a> was open source. Heard about it a year ago from a presentation.</p>
<ul>
<li>Github: <a href="https://github.com/excalidraw/excalidraw" class="uri">https://github.com/excalidraw/excalidraw</a></li>
<li>VS Code Extension: <a href="https://marketplace.visualstudio.com/items?itemName=pomdtr.excalidraw-editor" class="uri">https://marketplace.visualstudio.com/items?itemName=pomdtr.excalidraw-editor</a></li>
<li>just need to create a new file with a .excalidraw extension and open it. Excalidraw saves data in JSON.</li>
</ul>
<p>Also a list of public libraries/extensions: <a href="https://libraries.excalidraw.com/" class="uri">https://libraries.excalidraw.com/</a></p>



 ]]></description>
  <category>tools</category>
  <guid>https://lawwu.github.io/til/posts/2024-08-13-excalidraw/</guid>
  <pubDate>Tue, 13 Aug 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>VS Code &amp; Port Forwarding</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-07-26-vscode-port-forwards/</link>
  <description><![CDATA[ 





<p>Today I ran into this issue where I wasn’t able to call some Azure Functions I was developing locally. When Remote SSH’ing into a VM using VS Code, if you start a webserver on that machine, the port is automatically forwarded to your machine’s localhost. Since my webserver was using the same port 7072, and forward to <code>localhost:7072</code>, the Azure Functions running locally also at <code>localhost:7072</code> were not accessible since calls to that address were being sent to the webserver (via the port forward).</p>
<p>Clicking the Ports tab in VS Code shows the port being Auto Forwarded:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://lawwu.github.io/til/posts/2024-07-26-vscode-port-forwards/port_forward.png" class="img-fluid figure-img"></p>
<figcaption>Port Forward</figcaption>
</figure>
</div>



 ]]></description>
  <category>IDE</category>
  <category>port forwarding</category>
  <guid>https://lawwu.github.io/til/posts/2024-07-26-vscode-port-forwards/</guid>
  <pubDate>Fri, 26 Jul 2024 00:00:00 GMT</pubDate>
</item>
<item>
  <title>Positron IDE from Posit</title>
  <dc:creator>Lawrence Wu</dc:creator>
  <link>https://lawwu.github.io/til/posts/2024-07-19-positron/</link>
  <description><![CDATA[ 





<p>Today I learned, <a href="https://posit.co/">Posit</a> (use to be RStudio) is working on a new data science IDE that looks like RStudio built on the same underlying code as VS Code: <a href="https://github.com/posit-dev/positron" class="uri">https://github.com/posit-dev/positron</a> built on Code OSS.</p>
<p>Also learned that “Code OSS” is another name for VS Code: <a href="https://github.com/microsoft/vscode" class="uri">https://github.com/microsoft/vscode</a>.</p>



 ]]></description>
  <category>software</category>
  <guid>https://lawwu.github.io/til/posts/2024-07-19-positron/</guid>
  <pubDate>Fri, 19 Jul 2024 00:00:00 GMT</pubDate>
</item>
</channel>
</rss>
